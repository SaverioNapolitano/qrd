@inproceedings{vaswani2017attention,
  title={Attention Is All You Need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
@article{Hadi_2025,
title={Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects},
url={http://dx.doi.org/10.36227/techrxiv.23589741.v8},
DOI={10.36227/techrxiv.23589741.v8},
publisher={Institute of Electrical and Electronics Engineers (IEEE)},
author={Hadi, Muhammad Usman and Tashi, Qasem Al and Qureshi, Rizwan and Shah, Abbas and Muneer, Amgad and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Hassan, Syed Zohaib and Shoman, Maged and Wu, Jia and Mirjalili, Seyedali and Shah, Mubarak},
year={2025},
month=feb
}
@article{Annepaka2025,
  author    = {Annepaka, Yadagiri and Pakray, Partha},
  title     = {Large language models: a survey of their development, capabilities, and applications},
  journal   = {Knowledge and Information Systems},
  volume    = {67},
  number    = {3},
  pages     = {2967--3022},
  year      = {2025},
  month     = mar,
  doi       = {10.1007/s10115-024-02310-4},
  url       = {https://doi.org/10.1007/s10115-024-02310-4}
}
@INPROCEEDINGS{10386743,
  author={Wu, Jiayang and Gan, Wensheng and Chen, Zefeng and Wan, Shicheng and Yu, Philip S.},
  booktitle={2023 IEEE International Conference on Big Data (BigData)}, 
  title={Multimodal Large Language Models: A Survey}, 
  year={2023},
  volume={},
  number={},
  pages={2247-2256},
  keywords={Surveys;Diversity reception;Focusing;Companies;Big Data;Data models;Task analysis;modalities;language models;multimodal models;large models;survey},
  doi={10.1109/BigData59044.2023.10386743}
}
@ARTICLE{10720163,
  author={Shao, Minghao and Basit, Abdul and Karri, Ramesh and Shafique, Muhammad},
  journal={IEEE Access}, 
  title={Survey of Different Large Language Model Architectures: Trends, Benchmarks, and Challenges}, 
  year={2024},
  volume={12},
  number={},
  pages={188664-188706},
  keywords={Surveys;Transformers;Benchmark testing;Encoding;Large language models;Adaptation models;Market research;Decoding;Training;Computational modeling;Large language models (LLMs);Transformer architecture;generative models;survey;multimodal learning;deep learning;natural language processing (NLP)},
  doi={10.1109/ACCESS.2024.3482107}
}
@InProceedings{pmlr-v139-radford21a,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}
@article{ABDULGALIL2025100159,
title = {Next-generation image captioning: A survey of methodologies and emerging challenges from transformers to Multimodal Large Language Models},
journal = {Natural Language Processing Journal},
volume = {12},
pages = {100159},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100159},
url = {https://www.sciencedirect.com/science/article/pii/S2949719125000354},
author = {Huda Diab Abdulgalil and Otman A. Basir},
keywords = {Natural Language Processing (NLP), Computer Vision (CV), Visual understanding, Image captioning, Multimodal learning, Transformer, Multimodal Large Language Models (MLLMs)},
abstract = {The widespread availability of visual data on the Internet has fueled a significant interest in image-to-text captioning systems. Automated image captioning remains a challenging multimodal analytics task, integrating advances in both Computer Vision (CV) and Natural Language Processing (NLP) to understand image content and generate semantically meaningful textual descriptions. Modern deep learning-based approaches have supplanted traditional approaches in image captioning, leading to more efficient and sophisticated models. The development of attention mechanisms and transformer-based architectures has further enhanced the modeling of both language and visual data. Despite these gains, challenges such as long-tailed object recognition, bias in training data, and shortcomings in evaluation metrics constrain the capabilities of current models. Furthermore, an important breakthrough has been made with the recent emergence of Multimodal Large Language Models (MLLMs). By incorporating textual and visual data, MLLMs provide improved captioning flexibility, generative capabilities, and reasoning. However, these models introduce new challenges, including faithfulness, grounding, and computational cost. Although relatively few studies have comprehensively surveyed these developments, this paper provides a thorough analysis of Transformer-based captioning approaches, investigates the shift to MLLMs, and discusses associated challenges and opportunities. We also present a performance comparison of the latest models on the MS-COCO benchmark and conclude with perspectives on potential future research directions.}
}
@INPROCEEDINGS{11094992,
  author={Wang, Andrew Z. and Ge, Songwei and Karras, Tero and Liu, Ming-Yu and Balaji, Yogesh},
  booktitle={2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation}, 
  year={2025},
  volume={},
  number={},
  pages={28575-28585},
  keywords={Training;Analytical models;Computational modeling;Scalability;Large language models;Pipelines;Semantics;Text to image;Cognition;Pattern recognition},
  doi={10.1109/CVPR52734.2025.02661}
}
@Article{app131911103,
AUTHOR = {Ondeng, Oscar and Ouma, Heywood and Akuon, Peter},
TITLE = {A Review of Transformer-Based Approaches for Image Captioning},
JOURNAL = {Applied Sciences},
VOLUME = {13},
YEAR = {2023},
NUMBER = {19},
ARTICLE-NUMBER = {11103},
URL = {https://www.mdpi.com/2076-3417/13/19/11103},
ISSN = {2076-3417},
ABSTRACT = {Visual understanding is a research area that bridges the gap between computer vision and natural language processing. Image captioning is a visual understanding task in which natural language descriptions of images are automatically generated using vision-language models. The transformer architecture was initially developed in the context of natural language processing and quickly found application in the domain of computer vision. Its recent application to the task of image captioning has resulted in markedly improved performance. In this paper, we briefly look at the transformer architecture and its genesis in attention mechanisms. We more extensively review a number of transformer-based image captioning models, including those employing vision-language pre-training, which has resulted in several state-of-the-art models. We give a brief presentation of the commonly used datasets for image captioning and also carry out an analysis and comparison of the transformer-based captioning models. We conclude by giving some insights into challenges as well as future directions for research in this area.},
DOI = {10.3390/app131911103}
}


